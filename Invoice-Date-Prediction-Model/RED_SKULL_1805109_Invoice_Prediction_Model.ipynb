{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RED_SKULL_1805109_Invoice_Prediction_Model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa_CkPA5JESt"
      },
      "source": [
        "#INVOICE PAYMENT DATE PREDICTION MODEL\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgEQ7almNXTN"
      },
      "source": [
        "# 1.1 Loading the given data set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "id": "dwTmj8SANlUx",
        "outputId": "79759e04-46ee-4d68-e9ee-833856f0806c"
      },
      "source": [
        "import pandas as pd\n",
        "original_main_data=pd.read_csv(\"/content/drive/MyDrive/1805109.csv\")\n",
        "original_main_data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-278b63014d24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_main_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/1805109.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0moriginal_main_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/1805109.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWzjJKHNOFsf"
      },
      "source": [
        "#1.2 Getting to know our Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azRIfGgpPSjt"
      },
      "source": [
        "# Finding the total number of rows or entries in the data set\n",
        "len(original_main_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib_I-whmXnKa"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gjwhwrZ66E8"
      },
      "source": [
        "# Getting first look at the data\n",
        "original_main_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAGYUpz87PRC"
      },
      "source": [
        "# Checking for NULL values\n",
        "\n",
        "original_main_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-phBrW-bO8I-"
      },
      "source": [
        "INFERENCE :\n",
        "\n",
        "       1. The column \" area_business \" is completly empty\n",
        "\n",
        "       2. The column \" clear date \" has some null values. These are intentionally null as these are the dates we have to predict.\n",
        "\n",
        "       3. Most of the date columns are of type Object"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVlXV1s6Q5tx"
      },
      "source": [
        "# 2.1 Basic Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZTSuZcN7aEB"
      },
      "source": [
        "# Converting the Object type date columns to Date_Time objects\n",
        "\n",
        "\n",
        "from datetime import datetime\n",
        "\n",
        "original_main_data['posting_date']=pd.to_datetime(original_main_data['posting_date'])\n",
        "\n",
        "original_main_data['document_create_date']=pd.to_datetime(original_main_data['document_create_date'],format='%Y%m%d')\n",
        "\n",
        "original_main_data['document_create_date.1']=pd.to_datetime(original_main_data['document_create_date.1'],format='%Y%m%d')\n",
        "\n",
        "original_main_data['due_in_date']=pd.to_datetime(original_main_data['due_in_date'],format='%Y%m%d')\n",
        "\n",
        "original_main_data['baseline_create_date']=pd.to_datetime(original_main_data['baseline_create_date'],format='%Y%m%d')\n",
        "\n",
        "original_main_data['due_in_date']=pd.to_datetime(original_main_data['due_in_date'])\n",
        "\n",
        "original_main_data['clear_date']=pd.to_datetime(original_main_data['clear_date'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyTszkyTYO9O"
      },
      "source": [
        "# Verifying our Conversions\n",
        "\n",
        "original_main_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkByzYECRXa9"
      },
      "source": [
        "NOTE :\n",
        "\n",
        "       1. The column \"area_business\" is completly NULL.It is not feasible for *NULL imputation* .\n",
        "\n",
        "       2. Hence I ma dropping it as it will not give any useful information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDpgKN09YO2D"
      },
      "source": [
        "# Dropping column 'area_business'\n",
        "original_main_data.drop('area_business',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsFmNBhkSgNn"
      },
      "source": [
        "# 2.2 Checking for anamolies in our data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGhKCXXXTMUO"
      },
      "source": [
        "NOTE :\n",
        "\n",
        "     1. From the given data dictionary we have,\n",
        "       \n",
        "        * document_create_date is the date on which the invoice document was created \n",
        "\n",
        "        * clear_date is the date when the invoice was cleared.\n",
        "\n",
        "        Thus,it doesen't make entries to have clear_date before document_create_date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCju7CTkYOzY"
      },
      "source": [
        "#Checking for the above condition\n",
        "\n",
        "len(original_main_data.loc[original_main_data['document_create_date.1'] > original_main_data['clear_date']]) \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iZ4rteNUO1F"
      },
      "source": [
        "Similarly, below I have checked for columns like \n",
        "\n",
        "               1. due_in_date\n",
        "\n",
        "               2. posting_date\n",
        "\n",
        "               3. baseline_create_date\n",
        "\n",
        "  These columns should also not contain dates before document_create_date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuqBpyg2vm_f"
      },
      "source": [
        "len(original_main_data.loc[original_main_data['document_create_date.1'] > original_main_data['due_in_date']])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00Uw2Q57YOwm"
      },
      "source": [
        "len(original_main_data.loc[original_main_data['document_create_date.1'] > original_main_data['posting_date']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HphXhkB0YOtV"
      },
      "source": [
        "len(original_main_data.loc[original_main_data['document_create_date.1'] > original_main_data['baseline_create_date']])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7_vmXCXUzFa"
      },
      "source": [
        "Below is the percenatge of these rows with anomalies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfxiaONBYOp7"
      },
      "source": [
        "# number of abnormal data rows as as percentage\n",
        "\n",
        "(134+2154)/len(original_main_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qoz8Hq7VDCX"
      },
      "source": [
        "Now as these rows constiture of only about 4% of our data-set, It is better to drop them\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHQb8kvbijYY"
      },
      "source": [
        "#Dropping the abnormal rows\n",
        "original_main_data.drop(original_main_data[original_main_data['document_create_date.1'] > original_main_data['due_in_date']].index,inplace=True,axis=0)\n",
        "original_main_data.drop(original_main_data[original_main_data['document_create_date.1'] > original_main_data['baseline_create_date']].index,inplace=True,axis=0)\n",
        "len(original_main_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qchPi02OWXFh"
      },
      "source": [
        "# 3.1 DATA SPLITTING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6srAB7goWwcW"
      },
      "source": [
        "From observation ,my data is time based hence is to be split according to the given dates in ascending order.\n",
        "\n",
        "1. The data is  splitted  into training and test sets based on a SPLIT_DATE.\n",
        "\n",
        "2. The entries haveing data before this date belong to train set and after the split date belong to test set "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X7LxDE8K7Jo"
      },
      "source": [
        "#Splitting the data set into train and test sets\n",
        "\n",
        "# The entries whicch do not have a clearing date are the ons we have to predict.\n",
        "\n",
        "# Here I put those entries into test_set and the remaining into train set\n",
        "training_data=original_main_data.loc[original_main_data['clear_date'].isna()==False,:]\n",
        "test_data=original_main_data.loc[original_main_data['clear_date'].isna()==True,:]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoUz1HZKLyFi"
      },
      "source": [
        "# Training Data\n",
        "training_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s8M_j358MhOQ"
      },
      "source": [
        "# Test Data\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpsQzjVbNGHB"
      },
      "source": [
        "training_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqnCIWJkOvZ7"
      },
      "source": [
        "test_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GWbeLybX6FR"
      },
      "source": [
        "Now testing for null values in training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGsweCszOzpb"
      },
      "source": [
        "training_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZz5SAmrYXUr"
      },
      "source": [
        "NOTE :\n",
        "       1. There are still rows where the column \"invoice_id\" contains some NULL values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lhM73LIYZd1N"
      },
      "source": [
        "# 4.1 Exploratory Data Analysis (E.D.A)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI3PPDYURMc_"
      },
      "source": [
        "# Note the currency is of different types for different invoices\n",
        "test_data['invoice_currency'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVvc32jYSgmU"
      },
      "source": [
        "NOTE TO SELF :\n",
        "\n",
        "By now Data has been split by date into training and testing sets (first 70-30%)split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umKO5W4YT0c-"
      },
      "source": [
        "original_main_data[['clear_date','posting_date','document_create_date','due_in_date','baseline_create_date']].head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_Oefny1fkP4"
      },
      "source": [
        " NOTE:-\n",
        "\n",
        "     <#> Target value is the Column named 'due_in_date'\n",
        "\n",
        "     <#> Bucket value can be calculated by the following :\n",
        "              \n",
        "              'aging_bucket'='clear_date' - 'due_in_date'\n",
        "              "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLbyvFlOXGY8"
      },
      "source": [
        "training_data.groupby('invoice_currency').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLJxVajAaL-h"
      },
      "source": [
        "Below we have our first look at the test_data.\n",
        "\n",
        "*   The test data has 4188 entries all of which have \"clear_date\" as null\n",
        "\n",
        "*   These are the entries we have to predict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSVZ-Xeua3lg"
      },
      "source": [
        "len(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KPibw2_Ty7y"
      },
      "source": [
        "test_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4w-dco_Z71A"
      },
      "source": [
        "training_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znRaYlD-doAK"
      },
      "source": [
        "The column \"Invoice_id\" is an id column .\n",
        "\n",
        "\n",
        "*   Since it is not feasible to impute these values for the time being i am dropping it\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deob9k95aA4p"
      },
      "source": [
        "# Note invoice_id is missing 6 values, for time being I am dropping it.\n",
        "training_data[pd.isnull(training_data['invoice_id'])==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQWvwW6sbQ5n"
      },
      "source": [
        "# these are the 6 empty rows in the training data-set\n",
        "# Our test data contains no empty rows (confirmed)\n",
        "\n",
        "train_data_new=training_data[pd.isnull(training_data['invoice_id'])!=True]\n",
        "train_data_new"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV391CVIhUUo"
      },
      "source": [
        "train_data_new.isna().sum()\n",
        "\n",
        "# Thus all null values are now finally removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GEd0pKneI8Q"
      },
      "source": [
        "As seen earlier, the currency column has two values.\n",
        "\n",
        "In other words they are not in the same unit."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRM7sWF0h4pE"
      },
      "source": [
        "# Now solving the currency issue (Converting all CAD to USD throught)\n",
        "\n",
        "\n",
        "train_data_new.groupby('invoice_currency').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPcZKmB7iOem"
      },
      "source": [
        "train_data_new.loc[train_data_new['invoice_currency']=='CAD'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwEtJE7Ni693"
      },
      "source": [
        "# Conversion from CAD to USD \n",
        "\n",
        "# i CAD =0.79 CAD\n",
        "import numpy as np\n",
        "train_data_1=train_data_new\n",
        "train_data_1['total_open_amount']=np.where(train_data_1.invoice_currency=='CAD',\n",
        "                                             train_data_1.total_open_amount*0.79,\n",
        "                                           train_data_1.total_open_amount)\n",
        "train_data_1.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByKn1jlRq1Yv"
      },
      "source": [
        "print(\"Data brfore conversion\")\n",
        "original_main_data.loc[45590,['invoice_currency','total_open_amount']]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPaW7wnnvCgx"
      },
      "source": [
        "train_data=train_data_new\n",
        "train_data.loc[45590,['invoice_currency','total_open_amount']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCc7uhSje4va"
      },
      "source": [
        "As evident from above, all CAD have been converted tro USD and stored in train_data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITHK4A7evWwl"
      },
      "source": [
        "train_data.info()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnBYhLV7zKxc"
      },
      "source": [
        "# Now we sort the training data based on invoice posting date\n",
        "\n",
        "train_data.sort_values('document_create_date.1',inplace=True)\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UCB2qZ1mMnJ"
      },
      "source": [
        "# 4.2 CONSTANT COLUMNS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sn8TyiG_k1rJ"
      },
      "source": [
        "Taking a look at the \"document_type\" column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxZc0hJtz5G2"
      },
      "source": [
        "train_data['document type'].unique()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTrvH_YHxvVB"
      },
      "source": [
        "train_data.groupby(\"document type\").count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieM-tj2UlGQS"
      },
      "source": [
        " This 'document type' column has all same values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-B82QTeAm5pB"
      },
      "source": [
        "train_data.groupby(\"isOpen\").count()\n",
        "# isOpen values are all same"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_oD8AFkm9Pp"
      },
      "source": [
        "The column \"is_open\" also conatains same values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "423Xjer8l7qf"
      },
      "source": [
        "# 4.3 DUPLICATE COLUMNS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYYjBeHLlVX7"
      },
      "source": [
        "# Checking for duplicate columns\n",
        "train_data.loc[train_data.invoice_id!=train_data.doc_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGHH469RlyTx"
      },
      "source": [
        "Hence we can infer that \"invoice_id\" and \"doc_id\" are duplicate columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtDWHN3QnqC2"
      },
      "source": [
        "'invoce_id' and 'doc_id' are giving the same infomation \n",
        "\n",
        "So new info is derived from either of them \n",
        "\n",
        "So I can drop one of them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9Hh3bcIrcVI"
      },
      "source": [
        "train_data.groupby(\"cust_payment_terms\").count()  #esssential features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vT423DW8r6zw"
      },
      "source": [
        "'isOpen' column is all 0 because this is test data and all invoives \n",
        "are unique.\n",
        "\n",
        "This column also provides no new additional info.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1nmB43F4xOye"
      },
      "source": [
        "train_data['posting_id'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU4aSbFU0rHW"
      },
      "source": [
        "The column posting_id is also an constant column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-lSacUC029K"
      },
      "source": [
        "train_data.loc[train_data['document_create_date.1']!=train_data['baseline_create_date'],:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwgBRadHowJs"
      },
      "source": [
        "These two columns are different namely document_create_date.1 and baseline_create_date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Je0arV4wGrf"
      },
      "source": [
        " <#> Target value is the Column named 'due_in_date'\n",
        " \n",
        " <#> Bucket value can be calculated by the following :\n",
        " \n",
        "          'aging_bucket'='clear_date' - 'due_in_date"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F35Q9Pt4nROJ"
      },
      "source": [
        "train_data['clear_date']=pd.to_datetime(train_data['clear_date'])\n",
        "train_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSr0mL_nxZGh"
      },
      "source": [
        "train_data['due_in_date']=pd.to_datetime(train_data['due_in_date'])\n",
        "#train_data['due_in_date']=train_data['due_in_date'].dt.date\n",
        "train_data['delay']=(train_data['clear_date']-train_data['due_in_date'])\n",
        "\n",
        "train_data['delay']=(train_data['delay']/ np.timedelta64(1, 'D')).astype(int)\n",
        "\n",
        "train_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tICApBxMruPG"
      },
      "source": [
        "# Splitting into Train , Validation_1 and Validation_2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWUZ0rTJopCO"
      },
      "source": [
        "Now I am spliting the train data into train : Val 1 :Val 2 = 7:15:15\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGkcbM7CpFnw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBwRoGDKpAdk"
      },
      "source": [
        "len(train_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzIfxIV4pccs"
      },
      "source": [
        "final_train_data,val=np.split(train_data,[int(.7*len(train_data))])\n",
        "print(\"Length of final_train_data is \",len(final_train_data),\n",
        "      \"\\n Length of val is \",len(val))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcbz75GjsGKD"
      },
      "source": [
        "val1,val2=np.split(val,[int(.5*len(val))])\n",
        "print(\"Length of val1 is \",len(val1),\n",
        "      \"\\nLength of val2 is \",len(val2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkPCud6XpcKf"
      },
      "source": [
        "# Check for missing rows\n",
        "\n",
        "30556+6548*2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fqoYP0s9s-J6"
      },
      "source": [
        "Hence the data is effectively split witout missing any value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UQDgGtx-ozN"
      },
      "source": [
        "# Creating Dictionary which will later be used for mapping\n",
        "# This is to be done before splitting into x and y\n",
        "BC_DICT=final_train_data.groupby('business_code')['delay'].mean().to_dict()\n",
        "BC_DICT"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4McCXYxApcFM"
      },
      "source": [
        "Thus , now the split into train_final,validation-1 and valdation_2 has been done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cS9qKDT6xgau"
      },
      "source": [
        "Now Splitting the data into X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7N6SMbUH0ZPz"
      },
      "source": [
        "X_train = final_train_data.copy()\n",
        "X_train.drop('delay', axis=1, inplace=True)\n",
        "y_train = final_train_data['delay']\n",
        "final_train_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-L37yz2zpIpl"
      },
      "source": [
        "final_train_data.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sYSIJ6b0ZNV"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_KXlFm20ZKP"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8CRYsFdz0ZHU"
      },
      "source": [
        "Thus,the data is split into X_train and y_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cib5HJcY0ZEj"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqGr3Lur7-ZC"
      },
      "source": [
        "# 5.1 Data Visualization and Pattern Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poSk4meO8PKI"
      },
      "source": [
        "Testing for patterns in data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoR05COm8Vgv"
      },
      "source": [
        "Analysis of due_in_date with delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qshLotTQ0Yxc"
      },
      "source": [
        "\n",
        "days=X_train['due_in_date'].dt.day\n",
        "\n",
        "# Define Day maping function\n",
        "def day_to_bin(day):\n",
        "  if day<=7:\n",
        "    return 1\n",
        "  elif day>7 and day<=14:\n",
        "    return 2\n",
        "  elif day>14 and day<=21:\n",
        "    return 3\n",
        "  else:\n",
        "    return 4\n",
        "\n",
        "day_bins=np.array(days.map(day_to_bin))\n",
        "np.unique(day_bins)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLlAOJuLBihQ"
      },
      "source": [
        "Effect of Months on delay:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLPWgCuu0Yr3"
      },
      "source": [
        "#Trying monthsas feature\n",
        "due_months=X_train['due_in_date'].dt.month\n",
        "import matplotlib.pyplot as plt\n",
        "fig,ax=plt.subplots(figsize=(7,3))\n",
        "plt.xlabel('months')\n",
        "ax.hist(due_months,bins=[0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyOd7PY2o2Mb"
      },
      "source": [
        "From here It can be infered that the month in which the client makes payment may be a useful feature as no two months have the same no of due clearence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4bx6lZUBpr4"
      },
      "source": [
        "Effect of Amount on delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DljI7-lM0k3W"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(9,5))\n",
        "plt.xlabel('total_open_amount')\n",
        "plt.ylabel('delay')\n",
        "x=(final_train_data['total_open_amount'])\n",
        "y=final_train_data['delay']\n",
        "plt.scatter(x,y,c=\"blue\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmgCeuXEqq5A"
      },
      "source": [
        "INFERENCE:\n",
        "\n",
        "Clients with less open_amount tebd to delay more than the ines with hogher total_open_amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T86qV3O7HPrV"
      },
      "source": [
        "train_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuUOZtOm1vTA"
      },
      "source": [
        "train_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsph3hqByBG_"
      },
      "source": [
        "Examining the distribution of total_open_amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2rsRqtTyJvB"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(a=X_train['total_open_amount'], kde=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTwm6TgAyM_P"
      },
      "source": [
        "The distribution is right skewed as seen from graph\n",
        "\n",
        "I will later use log transform to make it as close to a normal distribution as follows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uJsiRpwBuHw"
      },
      "source": [
        "Range of Delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNc3R_fAF0NW"
      },
      "source": [
        "# RAnge of Delay\n",
        "range=np.ptp(train_data['delay'])\n",
        "range"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHPsOne5HRX-"
      },
      "source": [
        "train_data['delay'].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBpQpcM3ei7K"
      },
      "source": [
        "train_data['delay'].mean()/365\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnLitRseB38J"
      },
      "source": [
        "Effect of business year on Delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X-g35s4r-6s"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x=train_data.loc[:100,'buisness_year']\n",
        "y=train_data.loc[:100,'delay']\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "plt.xlabel('buisness_year')\n",
        "plt.ylabel('delay')\n",
        "plt.scatter(x,y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-Am50oryydL"
      },
      "source": [
        "train_data.groupby('buisness_year').sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-oGtOWby75K"
      },
      "source": [
        "# Scaling total_open_amount by a factor of 1000\n",
        "train_data['total_amount_by_1000']=train_data['total_open_amount']/1000\n",
        "train_data.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPT4etpIr0n7"
      },
      "source": [
        "A Plot between cust_number and total_amount"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eEhpIDp1bWR"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.scatter(train_data.loc[:,'cust_number'],train_data.loc[:,'total_amount_by_1000'])\n",
        "plt.xlabel('cust_number')\n",
        "plt.ylabel('total_amount')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5riEbsCOFi"
      },
      "source": [
        "Effect of Customer Payment Terms on Delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqJykPVb2FM7"
      },
      "source": [
        "# Types of terms in data\n",
        "terms=train_data.cust_payment_terms.unique()\n",
        "terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mmsfbg4ckeD"
      },
      "source": [
        "# Visual Plot\n",
        "df=pd.DataFrame(train_data.groupby('cust_payment_terms')['total_amount_by_1000'].mean()).reset_index()\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(9,6))\n",
        "plt.xlabel('cust_payment_terms')\n",
        "plt.ylabel('total_amount(scaled)')\n",
        "ax.plot(df['cust_payment_terms'],df['total_amount_by_1000'])\n",
        "fig.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmMM5QK5GKsa"
      },
      "source": [
        "Information given by this is not sufficient so moving on to bar-plot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdnO_84qU_dc"
      },
      "source": [
        "# Plotting bar-plot using seaborn\n",
        "sns.set(style=\"whitegrid\")\n",
        "fig=plt.figure(figsize=(30,20))\n",
        "terms=train_data.groupby('cust_payment_terms')['delay'].sum()\n",
        "sns.barplot(y=terms.index,x=terms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H4WxiriFXqW"
      },
      "source": [
        "The above plot shows customer_payment_terms is an essential factor for delay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WejxcpHHYefI"
      },
      "source": [
        "Hence confirmed that customer payment terms is an Nominal Column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOfKnkziGAso"
      },
      "source": [
        "# 6.1 Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjHpmgFGZ0Lg"
      },
      "source": [
        "Now doing One Hot encoding for Customer Payment Terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBBD4GWdlbYB"
      },
      "source": [
        "Sadly One hot encoding did not work , It created 68 columns\n",
        "\n",
        "Now moving On to Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdrGJuMp2jkv"
      },
      "source": [
        "# Creating a new column that is Invoice Currency code range(0,1)\n",
        "\n",
        "X_train['inv_curr_code']=np.where(X_train.invoice_currency=='CAD',0,1)\n",
        "X_train.loc[X_train.invoice_currency=='CAD',:]\n",
        "\n",
        "# Now we can drop the column invoice_currency column\n",
        "X_train.drop('invoice_currency',axis=1,inplace=True)\n",
        "X_train.head()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtYlGP7A3hI8"
      },
      "source": [
        "Now encoding the column 'cust_payment_terms' using Target Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPePZMlL5eiW"
      },
      "source": [
        "len(X_train.cust_payment_terms.unique())\n",
        "\n",
        "# These are the number of unique Labels in the cust_payment_terms column\n",
        "# These are to be encoded\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXK6jnzq5tDb"
      },
      "source": [
        "X_train.groupby('cust_payment_terms').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_jyzAsMuj0b"
      },
      "source": [
        "NOTE:\n",
        "\n",
        "\n",
        "\n",
        "*   As from our graph we nmotice that the customer payment terms gighly affect the delay.\n",
        "*   Thus for this I created a new variable \"avg_delay_as_per_terms\"\n",
        "*   It is defined as the median of all the customer who agreed  on the same terms\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7ZUIvYd6ChV"
      },
      "source": [
        "# TARGET ENCODING\n",
        "\n",
        "X_train['target'] = y_train # adding it again for demonstration of target encoding\n",
        "\n",
        "target_mapper = X_train.groupby('cust_payment_terms')['target'].median().to_dict()\n",
        "print(target_mapper)\n",
        "\n",
        "X_train.drop('target',axis=1,inplace=True) # Removing it back\n",
        "\n",
        "X_train['avg_delay_as_per_terms'] = X_train['cust_payment_terms'].map(target_mapper)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t2DsL7sP-FTp"
      },
      "source": [
        "Hence , now the target encoding of the column 'cust_payment_terms' has been done\n",
        "\n",
        "The new column which is supposed to replace ot is named avg_delay_as_per_terms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGF6KCGVCp0l"
      },
      "source": [
        "Now encoding Customer_name column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNW8uVuqvRkr"
      },
      "source": [
        "Now Encoding Customer Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whaRgkVgC_8e"
      },
      "source": [
        "# Unique labels for customer name\n",
        "len(X_train.name_customer.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUNiBl7BD8iX"
      },
      "source": [
        "# Target Encoding for customer_name\n",
        "temp=X_train.copy()\n",
        "temp['delay']=y_train\n",
        "col_map=temp.groupby('name_customer')['delay'].mean().to_dict()\n",
        "col_map\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN3LYjL_EX1G"
      },
      "source": [
        "Now merging this with the X_train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kz3aCtbHJcJ"
      },
      "source": [
        "'''\n",
        "new=X_train.merge(col_map,how='left',left_on=['name_customer'],right_on=['name_customer'])\n",
        "new.loc[45:50,:]\n",
        "'''\n",
        "X_train['cust_name(ENC)']=X_train['name_customer'].map(col_map)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssPHfIsHKt7s"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oUQhfJfvf0o"
      },
      "source": [
        "Thus by now customer_names have been encoded as \"cust_name(ENC)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMUPe7Rgv-K7"
      },
      "source": [
        "Business_year column is already in float and requires no changes.\n",
        "\n",
        "This  column is also essential as delay depends on business year as per graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbNZkhZfLEoi"
      },
      "source": [
        "X_train.groupby('buisness_year').size()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HCQgWYzQh0i"
      },
      "source": [
        "len(X_train.business_code.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Lr0kV-tlz_I"
      },
      "source": [
        "Encoding Customer Number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9RP1V8-mS8Z"
      },
      "source": [
        "# Unique customer numbers\n",
        "len(X_train.cust_number.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xskeeO8qmW89"
      },
      "source": [
        "# Target Encoding for cust_number\n",
        "temp=X_train.copy()\n",
        "temp['delay']=y_train\n",
        "MAPPER=temp.groupby('cust_number')['delay'].mean().to_dict()\n",
        "MAPPER"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRk1Cb9VmmqS"
      },
      "source": [
        "# mapping cust_number(ENC)\n",
        "X_train['cust_num(ENC)']=X_train['cust_number'].map(MAPPER)\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5LPi9DenD2x"
      },
      "source": [
        "Now ,\n",
        "       The column cust_number has  beeen encoded as 'cust_num(ENC)'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBA-u9Vfwn6n"
      },
      "source": [
        "# 7.1 Feature Engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmMIc7d9w0m8"
      },
      "source": [
        "Here I am creating a new feature called as \"time_given\"\n",
        "*   This feature denotes the time given to a particular client to clear his payment dues\n",
        "*   time_given=due_in_date - document_create_date.1\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_cVFF0K2oU3_"
      },
      "source": [
        "# Creating new feature\n",
        "X_train['time_given']=X_train['due_in_date']-X_train['document_create_date.1']\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJ-dYyYixWcY"
      },
      "source": [
        "The time difference is of type \"TimeDelta\"\n",
        "\n",
        "Now converting it to int type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0Ax8oWbsLS5"
      },
      "source": [
        "# Conversion from TimeDelta to int\n",
        "X_train['time_given']=(X_train['time_given']/ np.timedelta64(1, 'D')).astype(int)\n",
        "X_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIkov6Ti_LBj"
      },
      "source": [
        "Let us take a look at the scatter plot and visualize how it affects delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "La59Zyzn_Kcy"
      },
      "source": [
        "# Sctter plot of \n",
        "plt.scatter(y_train,X_train['time_given'],color='black')\n",
        "plt.xlabel(\"time_given\")\n",
        "plt.ylabel(\"delay\")\n",
        "plt.title(\"delay Vs. time_given\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iKeWK26U_qOu"
      },
      "source": [
        "INFERENCE:\n",
        "\n",
        "*   It is observed from the graph that if more time is given to a client , delay is less and vice-versa\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xUh7WSgy4Hm"
      },
      "source": [
        "The doc_id column has many unique values as  seen from below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk3hRgcOujlA"
      },
      "source": [
        "# analysis of unique values in doc_id\n",
        "len(X_train.doc_id.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7pVYQ9elzHFE"
      },
      "source": [
        "As stated earlier now we engineeer a new column to replace \"total_open_amount\" by using log transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zr0KRt6JHnPw"
      },
      "source": [
        "#Normalizing (log2 Transformation)\n",
        "X_train['total_open_amount_1']=np.log2(X_train['total_open_amount'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rDpbsrv5_-I"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.distplot(a=X_train['total_open_amount_1'], kde=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sjf5LoGqzTXx"
      },
      "source": [
        "This plot seems closer to  a normal distribution as compared to before"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NE-4sgYOIFED"
      },
      "source": [
        "# Target encoding Business Code\n",
        "def BC_MAPPER(key):\n",
        "  if key not in BC_DICT.keys():\n",
        "    return 2 #default value\n",
        "  else:\n",
        "    return BC_DICT[key]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p44JlT130BRD"
      },
      "source": [
        "Now below we create two new columns namely,\n",
        "\n",
        "\n",
        "*   business_code(ENC) --> encoded form of business_code\n",
        "*   due_month -->month of due_in_date\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_JM1DXyIFBX"
      },
      "source": [
        "# Calling Encoder\n",
        "X_train['business_code(ENC)']=X_train['business_code'].map(BC_MAPPER)\n",
        "# Creating new feature due_month\n",
        "X_train['due_month']=X_train['due_in_date'].dt.month"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIQtdhas0bFk"
      },
      "source": [
        "As business code is now in float we check its impact on delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Wu9LmWjsT98"
      },
      "source": [
        "# Impact of business code on delay\n",
        "import matplotlib.pyplot as plt\n",
        "plt.xlabel('business code')\n",
        "plt.ylabel('delay')\n",
        "plt.scatter(X_train['business_code(ENC)'],y_train,color='black')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOj73sI80jyf"
      },
      "source": [
        "NOTE\n",
        "\n",
        "From graph it is sceen that business codes encoded as higher integers tend to show less delay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQ_Lu4pVIE-g"
      },
      "source": [
        "now business code has been target encoded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEcZCqD1cVyi"
      },
      "source": [
        "# Creating feature due_day\n",
        "X_train['due_day']=X_train['due_in_date'].dt.day\n",
        "# Document create month\n",
        "X_train['doc_month']=X_train['document_create_date.1'].dt.month\n",
        "# Document create day\n",
        "X_train['doc_day']=X_train['document_create_date.1'].dt.day"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaZBVla60-sn"
      },
      "source": [
        "# 8.1 Feature Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixfSUz2X4na0"
      },
      "source": [
        "Filter Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RT9uZfoF11dG"
      },
      "source": [
        "# Using Filter methods \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import VarianceThreshold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEVDY1Fa6I1A"
      },
      "source": [
        "Using Variance threshold :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkwv-IFim0Aq"
      },
      "source": [
        "**For** using Variance Threshold all features should be numerical which is not currently the case as seen below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV70U5fanDOQ"
      },
      "source": [
        "X_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjKyjI9enIeP"
      },
      "source": [
        "So I copy X_train into a new df and drop all non-numeric columns so it can be passed to VAriance Threshold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bEcg7MHnIBt"
      },
      "source": [
        "train=X_train.copy()\n",
        "train.drop(['business_code', 'cust_number','name_customer','clear_date','posting_date','document_create_date', 'document_create_date.1', 'due_in_date' ,'document type','posting_id','baseline_create_date',\t'cust_payment_terms','isOpen'],axis=1,inplace=True)\n",
        "train.drop(['doc_id','invoice_id','total_open_amount'],axis=1,inplace=True)\n",
        "train.info()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Te0Kim_111Yd"
      },
      "source": [
        "#Passing a value of zero for the parameter will filter all the features with zero variance\n",
        "constant_filter = VarianceThreshold(threshold=0)\n",
        "#we need to simply apply this filter to our training set as shown in the following example\n",
        "constant_filter.fit(train)\n",
        "#the number of non-constant features.\n",
        "len(train.columns[constant_filter.get_support()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrhswFVcoWpH"
      },
      "source": [
        "Thus we can say that all the 12 columns are not constant in nature.\n",
        "\n",
        "This is also verified from below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va01Fixe11PE"
      },
      "source": [
        "#Checking for constant columns\n",
        "constant_columns = [column for column in train.columns\n",
        "                    if column not in train.columns[constant_filter.get_support()]]\n",
        "\n",
        "print(len(constant_columns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiXfLUALDDvr"
      },
      "source": [
        "Effect of invoice_currency code on delay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FOR1fVlBJhx"
      },
      "source": [
        "# Plot\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "sns.set_theme(style=\"ticks\", color_codes=True)\n",
        "train=X_train.copy()\n",
        "train['delay']=y_train\n",
        "sns.catplot(x=\"inv_curr_code\", y=\"delay\",kind=\"bar\", data=train)  \n",
        "sns.catplot(x=\"buisness_year\",y=\"delay\",jitter=False,data=train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_trmjcdNDUqm"
      },
      "source": [
        "INFERENCE--> Clients who use CAD as invoice_currency tend to delay more than those who use USD\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-RtJqKY8I6f"
      },
      "source": [
        "# 8.2 Corelation and Covariace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EhcuY8a7FGh"
      },
      "source": [
        "Covariance & Corelation using HeatMap"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p8LaSth67EzJ"
      },
      "source": [
        "#visualisation\n",
        "train=X_train.copy()\n",
        "train.drop(['business_code', 'cust_number','name_customer','clear_date','posting_date','document_create_date', 'document_create_date.1', 'due_in_date' ,'document type','posting_id','baseline_create_date',\t'cust_payment_terms','isOpen'],axis=1,inplace=True)\n",
        "corr=train.corr()\n",
        "plt.figure(figsize=(14,8))\n",
        "sns.heatmap(corr,annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6luMKdzL5vgv"
      },
      "source": [
        "This shows the covariance between different columns of the X_train data.\n",
        "\n",
        "*   The co-relation betweeen target and other columns should be high.\n",
        "*   The co-relation among other columns should be asa low as possible.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kl6CpTyaKqUk"
      },
      "source": [
        "# 9.1 Fitting into Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iqg_O7burKF0"
      },
      "source": [
        "Now finally fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yijETBGatCpp"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrC9mYy9tHPb"
      },
      "source": [
        "#Saving X_train to another variable (BACKUP)\n",
        "just_in_case=X_train.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcPPnV1atvvV"
      },
      "source": [
        "X_train.drop(['business_code', 'cust_number','name_customer','clear_date','posting_date','document_create_date', 'document_create_date.1', 'due_in_date' ,'document type','posting_id','baseline_create_date',\t'cust_payment_terms','isOpen'],axis=1,inplace=True)\n",
        "X_train.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25Y5eWm76e7c"
      },
      "source": [
        "X_train.drop(['doc_id','invoice_id','total_open_amount'],axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x09G0K-hvJ45"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU8hvXeWK7KV"
      },
      "source": [
        "# 9.2 Mapper Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyJZOgHbwf54"
      },
      "source": [
        "Now , time has come to make our pre-processing function to make out test data ready,\n",
        "for making predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJtqA5HE7rye"
      },
      "source": [
        "# Defining Mapper Functions\n",
        "\n",
        "def cpt_mapper(key):\n",
        "  '''\n",
        "  maps customer payment terms\n",
        "  '''\n",
        "  if key not in target_mapper.keys():\n",
        "    return X_train['avg_delay_as_per_terms'].mean()\n",
        "  else:\n",
        "    return target_mapper[key]\n",
        "\n",
        "def cn_mapper(key):\n",
        "  '''\n",
        "  maps customer name\n",
        "  '''\n",
        "  if key not in col_map.keys():\n",
        "    return X_train['cust_name(ENC)'].mean()\n",
        "  else:\n",
        "    return col_map[key]\n",
        "\n",
        "def cno_mapper(key):\n",
        "  '''\n",
        "  maps customer number\n",
        "  '''\n",
        "  if key not in MAPPER.keys():\n",
        "    return X_train['cust_num(ENC)'].mean()\n",
        "  else:\n",
        "    return MAPPER[key]\n",
        "\n",
        "def BC_MAPPER(key):\n",
        "  '''\n",
        "  maps business code\n",
        "  '''\n",
        "  if key not in BC_DICT.keys():\n",
        "    return 2 #default value\n",
        "  else:\n",
        "    return BC_DICT[key]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m4lmpjELIAp"
      },
      "source": [
        "# 9.3 Preprocessing Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqaqbj86x_0l"
      },
      "source": [
        "def preprocess(X_Set,y_Set):\n",
        "  '''\n",
        "  This function takes a data-set and preprocess it \n",
        "  exactly the same as X-train data\n",
        "  '''\n",
        "  # Creating a new column that is Invoice Currency code {0,1}\n",
        "\n",
        "  X_Set['inv_curr_code']=np.where(X_Set.invoice_currency=='CAD',0,1)\n",
        " \n",
        "  # Now we can drop the column invoice_currency column\n",
        "  X_Set.drop('invoice_currency',axis=1,inplace=True)\n",
        "  \n",
        "  # TARGET ENCODING\n",
        "  \n",
        "  # Customer payment terms\n",
        "\n",
        "\n",
        "  X_Set['avg_delay_as_per_terms']= X_Set['cust_payment_terms'].map(cpt_mapper)\n",
        "\n",
        "  # Customer Name\n",
        "\n",
        "  X_Set['cust_name(ENC)']=X_Set['name_customer'].map(cn_mapper)\n",
        "  \n",
        "  # Customer Number\n",
        "  X_Set['cust_num(ENC)']=X_Set['cust_number'].map(cno_mapper)\n",
        "\n",
        "  # Creating New column \" time_given \"\n",
        "\n",
        "  X_Set['time_given']=X_Set['due_in_date']-X_Set['document_create_date.1']\n",
        "  X_Set['time_given']=(X_Set['time_given']/ np.timedelta64(1, 'D')).astype(int)\n",
        "\n",
        "  # Log transforming total_open_amount\n",
        "  X_Set['total_open_amount_1']=np.log2(X_Set['total_open_amount'])\n",
        "\n",
        "  # Mapping business code as business_code(ENC)\n",
        "  X_Set['business_code(ENC)']=X_Set['business_code'].map(BC_MAPPER)\n",
        "\n",
        "  #Creting new feature due month\n",
        "  X_Set['due_month']=X_Set['due_in_date'].dt.month\n",
        "\n",
        "  # Creating feature due_day\n",
        "  X_Set['due_day']=X_Set['due_in_date'].dt.day\n",
        "  # Document create month\n",
        "  X_Set['doc_month']=X_Set['document_create_date.1'].dt.month\n",
        "  # Document create day\n",
        "  X_Set['doc_day']=X_Set['document_create_date.1'].dt.day\n",
        "\n",
        "  #Dropping Columns\n",
        "  X_Set.drop(['business_code', 'cust_number','name_customer','clear_date','posting_date','document_create_date', 'document_create_date.1', 'due_in_date' ,'document type','posting_id','baseline_create_date',\t'cust_payment_terms','isOpen'],axis=1,inplace=True)\n",
        "  X_Set.drop(['doc_id','invoice_id','total_open_amount'],axis=1,inplace=True)\n",
        "  \n",
        "  return X_Set,y_Set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qQ3Y9JcLT0A"
      },
      "source": [
        "# 10.1 Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66WIQeCyLdGq"
      },
      "source": [
        "Spliting validation sets into x and y"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSHaVdHGNMf2"
      },
      "source": [
        "# Validation 1\n",
        "X_Val1=val1.copy()\n",
        "y_Val1=X_Val1['delay']\n",
        "X_Val1.drop('delay',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwgqg1ocfHBN"
      },
      "source": [
        "#Validation 2\n",
        "X_Val2=val2.copy()\n",
        "y_Val2=X_Val2['delay']\n",
        "X_Val2.drop('delay',axis=1,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LcNWXf-SIGJ"
      },
      "source": [
        "# 10.2 Fitting the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feMgMgpEiEUU"
      },
      "source": [
        "#Fitting the model\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "clf=RandomForestRegressor()\n",
        "clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PwoFf9aMQ6p"
      },
      "source": [
        "Predicting on Validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yx31Gku9jVps"
      },
      "source": [
        "# Predicting values for Validation Sets\n",
        "x_val1,y_val1=preprocess(X_Val1,y_Val1)\n",
        "x_val2,y_val2=preprocess(X_Val2,y_Val2)\n",
        "val1_preds=clf.predict(x_val1)\n",
        "val2_preds=clf.predict(x_val2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAmVjE6voHbJ"
      },
      "source": [
        "# Creating a Evluation Function\n",
        "from sklearn.metrics import r2_score,mean_squared_error\n",
        "def evaluate_model(labels,predictions):\n",
        "  print(\"RMSE : \",mean_squared_error(labels,predictions,squared=False))\n",
        "  print(\"R^2 SCORE : \",r2_score(labels,predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azQqf5zbqc4a"
      },
      "source": [
        "#Evluating the RandomForest Refressor\n",
        "evaluate_model(y_val1,val1_preds)\n",
        "evaluate_model(y_val2,val2_preds)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_E1ysOfblc2g"
      },
      "source": [
        "\n",
        "# Trying Lssso Reggressor\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.Lasso(alpha=0.9)\n",
        "reg.fit(X_train,y_train)\n",
        "evaluate_model(y_val1,reg.predict(x_val1))\n",
        "evaluate_model(y_val2,reg.predict(x_val2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kom5Zd5cNH1y"
      },
      "source": [
        "\n",
        "#Trying RidgeRegression Model\n",
        "from sklearn import linear_model\n",
        "ridge = linear_model.Ridge(alpha=.1)\n",
        "ridge.fit(X_train,y_train)\n",
        "val1_preds=ridge.predict(x_val1)\n",
        "val2_preds=ridge.predict(x_val2)\n",
        "evaluate_model(y_val1,ridge.predict(x_val1))\n",
        "evaluate_model(y_val2,ridge.predict(x_val2))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta-lscLSjv0z"
      },
      "source": [
        "From my experiments I found that the random forest regressor gave the best base results for me"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTE_CSn0tHPC"
      },
      "source": [
        "\n",
        "\n",
        "*   Below is the list of all other models I tried .\n",
        "*   I found that the best result was given by RAndomForestRegressor()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwKmbSqzNHsV"
      },
      "source": [
        "\n",
        "#Trying SVC kernel='linear'\n",
        "from sklearn import svm\n",
        "regr = svm.SVR()\n",
        "regr.fit(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWX2tck8ZcuL"
      },
      "source": [
        "y2_preds=ridge.predict(x_val1)\n",
        "mean_squared_error(y_val2,y2_preds,squared=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3awYsLi8NHlz"
      },
      "source": [
        "\n",
        "#Trying out GradientBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "est = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1,\n",
        "    max_depth=1, random_state=0, loss='ls').fit(X_train, y_train)\n",
        "est.predict(x)\n",
        "y2_preds=ridge.predict(x)\n",
        "mean_squared_error(y2,y2_preds,squared=False)\n",
        "r2_score(y2,y_preds)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jP3JDm0qRV0s"
      },
      "source": [
        "'''\n",
        "#Lasso Variant\n",
        "from sklearn import linear_model\n",
        "reg = linear_model.LassoLars(alpha=.1)\n",
        "reg.fit(X_train,y_train)\n",
        "mean_squared_error(y2,y2_preds,squared=False)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5J-cEbmtvE-"
      },
      "source": [
        "# 10.3 HyperParameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1pIiL7_t7Mz"
      },
      "source": [
        "Fitting model with n_estimators=100"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flrNqq85uHkp"
      },
      "source": [
        "# Fitting \n",
        "clf=RandomForestRegressor(n_estimators=100)\n",
        "clf.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7SvTpxJuN3i"
      },
      "source": [
        "#Now making predictions on the same validation sets\n",
        "val1_pred=clf.predict(x_val1)\n",
        "val2_pred=clf.predict(x_val2)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfJrpRetucL3"
      },
      "source": [
        "# Testing the accuracy\n",
        "evaluate_model(y_val1,val1_preds)\n",
        "evaluate_model(y_val2,val2_preds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZj5hJJHwzGr"
      },
      "source": [
        "As we can see tuning is not giving better results so I have used the base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AwH_-w_XR65T"
      },
      "source": [
        "# 11.1 Making predictions on the test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf4YHrH3lrpb"
      },
      "source": [
        "Now I am making predictions on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2WEre00SAQC"
      },
      "source": [
        "df=test_data.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQGaW7DSl1Fq"
      },
      "source": [
        "It is to be noted that the test data is not yet processed and contains NULL values in the clear date column"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIlwPiXCl-hn"
      },
      "source": [
        "We need to fill these values with our predicted dates and finally bin them according to the following buckets.\n",
        "\n",
        " 0-15 days\n",
        "\n",
        " 16-30 days\n",
        "\n",
        " 31-45 days\n",
        "\n",
        " 46-60 days\n",
        "\n",
        " Greater than 60 days\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DbCK7tZqc1v"
      },
      "source": [
        "Now this is ready for preprocessing function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfsYm49cqyyn"
      },
      "source": [
        "# X_test and y_test creation\n",
        "y_test=[]\n",
        "X_test,y_test=preprocess(test_data,y_test)\n",
        "X_test.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0vFrSnnsNHP"
      },
      "source": [
        "Making Predictions Using our RandomForest Regressor model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VeOz6wPrR6w"
      },
      "source": [
        "y_test=clf.predict(X_test)\n",
        "y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD4eFaPfvDUg"
      },
      "source": [
        "Rounding float to int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zpeRWkYYn5pO"
      },
      "source": [
        "AS we are predicting the number of days of delay ,we require integers so here I round up the floating points to int"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZEY03NxsiMx"
      },
      "source": [
        "y_test_rounded=np.round(y_test).astype(int)\n",
        "y_test_rounded"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7j4R8rLZszuk"
      },
      "source": [
        "# Adding y_test to df\n",
        "df['delay']=y_test_rounded\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHjjIEOMwBTV"
      },
      "source": [
        "# Converting due_in_date to Date-time\n",
        "df['due_in_date']=pd.to_datetime(df['due_in_date'],format='%Y%m%d')\n",
        "# COnverting delay to time delta\n",
        "df['delay']=pd.to_timedelta(df['delay'], unit='D')\n",
        "# Creating new column clear_date\n",
        "df['clear_date']=df['due_in_date']+df['delay']\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fC4qgIrayb1b"
      },
      "source": [
        "Thus we can predict the clear date of an invoice"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhXGOyxYyBHV"
      },
      "source": [
        "# Function for bucketing\n",
        "from datetime import datetime, timedelta\n",
        "def bucketizer(delay):\n",
        "  '''\n",
        "  This function is used to map the delays to their \n",
        "  respective bucket\n",
        "  '''\n",
        "  delay=delay.total_seconds() / timedelta (days=1).total_seconds()\n",
        "  if delay <=15 :\n",
        "    return \"0-15 days\"\n",
        "  elif delay >=16 and delay <=30:\n",
        "    return \"15-30 days\"\n",
        "  elif delay >=31 and delay <=45:\n",
        "    return \"31-45 days\"\n",
        "  elif delay>=46 and delay<=60:\n",
        "    return \"46-60 days\"\n",
        "  else:\n",
        "    return \"greater than 60 days\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpFDYpqnzkU5"
      },
      "source": [
        "# Now Creating the Bucket column using this function\n",
        "df['bucket']=df['delay'].map(bucketizer)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVEXiJ2pz0VC"
      },
      "source": [
        "df.groupby('bucket').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7c1yAMi4CPg"
      },
      "source": [
        "Hence we have  bucketized all the test data entries and achieved our goal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqcemrfZw-cb"
      },
      "source": [
        "It is furthet clear from below,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQXjEOSB5MkG"
      },
      "source": [
        "df[['due_in_date','delay','clear_date','bucket']].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ievHEY-yx2yG"
      },
      "source": [
        "That's all !!! Thank You for ur time :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_VBqfgOzSIv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}